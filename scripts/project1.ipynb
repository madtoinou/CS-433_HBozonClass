{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '/home/toinou/course/ml/ml_proj1/data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "\n",
    "cols = ['DER_mass_MMC', 'DER_mass_transverse_met_lep',\n",
    "       'DER_mass_vis', 'DER_pt_h', 'DER_deltaeta_jet_jet',\n",
    "       'DER_mass_jet_jet', 'DER_prodeta_jet_jet', 'DER_deltar_tau_lep',\n",
    "       'DER_pt_tot', 'DER_sum_pt', 'DER_pt_ratio_lep_tau',\n",
    "       'DER_met_phi_centrality', 'DER_lep_eta_centrality',\n",
    "       'PRI_tau_pt', 'PRI_tau_eta', 'PRI_tau_phi', 'PRI_lep_pt',\n",
    "       'PRI_lep_eta', 'PRI_lep_phi', 'PRI_met', 'PRI_met_phi',\n",
    "       'PRI_met_sumet', 'PRI_jet_num', 'PRI_jet_leading_pt',\n",
    "       'PRI_jet_leading_eta', 'PRI_jet_leading_phi',\n",
    "       'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta',\n",
    "       'PRI_jet_subleading_phi', 'PRI_jet_all_pt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "\n",
    "def one_hot_jet_num(tX, idx=-1):\n",
    " \n",
    "    #extract the jet_num column\n",
    "    one_hot = tX[:,idx].astype(int)\n",
    "    jet_num = np.zeros((tX.shape[0], 4))\n",
    "    \n",
    "    #create one column for each possible valur of jet_num\n",
    "    jet_num[np.where(one_hot == 0),0] = 1\n",
    "    jet_num[np.where(one_hot == 1),1] = 1\n",
    "    jet_num[np.where(one_hot == 2),2] = 1\n",
    "    jet_num[np.where(one_hot == 3),3] = 1\n",
    "\n",
    "    #create empty array to contain the  new values \n",
    "    tX_hot = np.empty((tX.shape[0], 30+3))\n",
    "    \n",
    "    #fill the new array without jet_num\n",
    "    tX_hot[:,:-4] = tX[:,:-1]\n",
    "\n",
    "    #add the 4 news columns\n",
    "    tX_hot[:,[-4,-3,-2,-1]] = jet_num\n",
    "    return tX_hot\n",
    "\n",
    "\"\"\" -- Data cleaning -- \"\"\"\n",
    "\n",
    "def standardize(x):\n",
    "    \"\"\"Standardize along features axis, implemented to ignore jet_num features.\"\"\"\n",
    "    #Store ignored column\n",
    "    temp = x[:,-1]\n",
    "    x =  np.delete(x, -1, axis=1)\n",
    "    #Mean feature-wise\n",
    "    mean_col = np.mean(x, axis=0)\n",
    "    #Mean = 0\n",
    "    x = x - mean_col\n",
    "    #STD feature-wise\n",
    "    std_col = np.std(x, axis=0)\n",
    "    #Std = 1\n",
    "    x[:, std_col > 0] = x[:, std_col > 0] / std_col[std_col > 0]\n",
    "    x = np.c_[x, temp]\n",
    "    return x\n",
    "\n",
    "def clean(data):\n",
    "    \"\"\"Feature data cleaner, replace outliers with feature mean, build normal distribution\"\"\"\n",
    "    data = np.where(data == -999, np.nan, data)\n",
    "    \n",
    "    # Mean vector column-wise\n",
    "    col_mean = np.nanmean(data, axis=0)\n",
    "\n",
    "    # Find indices where Nan appears\n",
    "    inds = np.where(np.isnan(data))\n",
    "\n",
    "    # Place column means at each found index\n",
    "    data[inds] = np.take(col_mean, inds[1])\n",
    "    return standardize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move jet_num to the last position\n",
    "tX.T[[22, -1]] = tX.T[[-1, 22]]\n",
    "tX_hot = one_hot_jet_num(clean(tX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logreg(y, x, k_indices, k, gamma, degree, steps):\n",
    "    \n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    x_train = x[np.concatenate(([x_train for i,x_train in enumerate(k_indices) if i!=k]), axis=0)]\n",
    "    y_train = y[np.concatenate(([y_train for i,y_train in enumerate(k_indices) if i!=k]), axis=0)]\n",
    "    phi_tr = build_poly(x_train, degree)\n",
    "    phi_te = build_poly(x_test, degree)\n",
    "    #starting with random weights\n",
    "    w = np.random.rand(phi_tr.shape[1],1)\n",
    "    for step in range(steps):\n",
    "        #loss, w = learning_by_newton_method(y_train, phi_tr, w, gamma)\n",
    "        loss, w = learning_by_gradient_descent(y_train, phi_tr, w, gamma)\n",
    "    loss_tr = np.sqrt(2*compute_mse(y_train,phi_tr,w))\n",
    "    loss_te = np.sqrt(2*compute_mse(y_test,phi_te,w))\n",
    "    \n",
    "    return loss_tr, loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.52714288e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-9.07315481e-15  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-9.07315481e-15  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " ...\n",
      " [-9.07315481e-15  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-9.07315481e-15  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-9.07315481e-15  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_hot[:,-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 116. GiB for an array with shape (125000, 125000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-44c3ff47185f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m#loss_tr_k, loss_te_k, w = cross_validation(y, tX_hot, k_indices, k, lambda_, degree)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss_tr_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_te_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_logreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mrmse_tr_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_tr_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-252-62580de814c9>\u001b[0m in \u001b[0;36mcross_validation_logreg\u001b[0;34m(y, x, k_indices, k, gamma, degree, steps)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#loss, w = learning_by_newton_method(y_train, phi_tr, w, gamma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mloss_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcompute_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcompute_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi_te\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master/ma1/ml/ml_proj1/scripts/implementations.py\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master/ma1/ml/ml_proj1/scripts/implementations.py\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 116. GiB for an array with shape (125000, 125000) and data type float64"
     ]
    }
   ],
   "source": [
    "seed = 2415132\n",
    "degrees = [1]\n",
    "k_fold = 2\n",
    "lambdas = np.linspace(0, 1, 20)\n",
    "gammas = np.linspace(1,1.5,5)\n",
    "steps=3000\n",
    "\n",
    "# split data in k fold\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "# define lists to store the loss of training data and test data\n",
    "rmse_tr = []\n",
    "rmse_te = []\n",
    "ws = []\n",
    "\n",
    "# cross validation\n",
    "for degree in degrees:\n",
    "    #for lambda_ in lambdas:\n",
    "    for gamma in gammas:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            #loss_tr_k, loss_te_k, w = cross_validation(y, tX_hot, k_indices, k, lambda_, degree)\n",
    "            loss_tr_k, loss_te_k, w = cross_validation_logreg(y, tX_hot, k_indices, k, gamma, degree, steps)\n",
    "\n",
    "            rmse_tr_tmp.append(loss_tr_k)\n",
    "            rmse_te_tmp.append(loss_te_k)\n",
    "        ws.append(w)\n",
    "        #combine the loss over the folds\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(14,14)) # change this if needed\n",
    "\n",
    "#degree 1\n",
    "ax1 = f.add_subplot(311)\n",
    "#ax1.plot(lambdas, rmse_tr[:30], 'magenta', label=\"rmse_tr_deg1\")\n",
    "ax1.plot(lambdas, rmse_te[:20], 'brown', label=\"rmse_te_deg1\")\n",
    "\n",
    "#ax2 = f.add_subplot(312)\n",
    "#ax1.plot(lambdas, rmse_tr[30:60], 'yellowgreen', label=\"rmse_tr_deg2\")\n",
    "ax1.plot(lambdas, rmse_te[20:], 'dodgerblue', label=\"rmse_te_deg2\")\n",
    "\n",
    "#ax3 = f.add_subplot(313)\n",
    "#ax1.plot(lambdas, rmse_tr[60:90], 'lightskyblue', label=\"rmse_tr_deg3\")\n",
    "#ax1.plot(lambdas, rmse_te[60:90], 'k', label=\"rmse_te_deg3\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toinou/Documents/master/ma1/ml/ml_proj1/scripts/implementations.py:228: RuntimeWarning: divide by zero encountered in log\n",
      "  prediction = sigmoid(Z)\n",
      "/home/toinou/Documents/master/ma1/ml/ml_proj1/scripts/implementations.py:230: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/home/toinou/Documents/master/ma1/ml/ml_proj1/scripts/implementations.py:218: RuntimeWarning: overflow encountered in exp\n",
      "  s = 1.0/(1.0 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85690593 0.81811241 0.33628391 ... 0.0701682  0.02040992 0.48772608]\n",
      "[-1.94425306 -1.70436641 -0.4099008  ... -0.07275157 -0.02062108\n",
      " -0.6688958 ]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 1. 0.]\n",
      "[-inf   0.   0. ... -inf -inf   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 1. 1. ... 0. 1. 1.]\n",
      "[  0. -inf -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 1. 0.]\n",
      "[-inf   0.   0. ... -inf -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[  0.   0. -inf ...   0. -inf   0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 1. 1. ... 0. 1. 1.]\n",
      "[  0. -inf -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[  0.   0. -inf ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 1. 1. ... 0. 1. 1.]\n",
      "[  0. -inf -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[  0.   0. -inf ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 1. 1. ... 0. 1. 1.]\n",
      "[  0. -inf -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[  0.   0. -inf ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      " 5.24688084e-38 0.00000000e+00]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[  0.   0. -inf ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 0.]\n",
      "[  0.   0. -inf ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 1. 0. 0.]\n",
      "[-inf   0.   0. ... -inf   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      " 1.75227574e-07 0.00000000e+00]\n",
      "[           -inf  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      " -1.75227589e-07  0.00000000e+00]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 3.05105935e-227\n",
      " 0.00000000e+000 0.00000000e+000]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      " 1.59039636e-32 0.00000000e+00]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 0.00000000e+000\n",
      " 4.72362202e-101 0.00000000e+000]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[-inf   0. -inf ...   0.   0.   0.]\n",
      "[1. 0. 0. ... 0. 1. 0.]\n",
      "[-inf   0.   0. ...   0. -inf   0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[-inf   0.   0. ...   0.   0.   0.]\n",
      "[0. 0. 1. ... 0. 1. 1.]\n",
      "[  0.   0. -inf ...   0. -inf -inf]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-9dd2aeb14fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m#loss, w = learning_by_newton_method(y_train, x_train, w, gamma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mlosses_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mlosses_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master/ma1/ml/ml_proj1/scripts/implementations.py\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/master/ma1/ml/ml_proj1/scripts/implementations.py\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;34m\"\"\"compute the gradient of loss for logistic regression.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 656\n",
    "degrees = [1]\n",
    "gammas = [1.5]\n",
    "steps=2000\n",
    "ratio = .8\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(tX_hot, y, ratio, seed)\n",
    "\n",
    "# define lists to store the loss of training data and test data\n",
    "ws = []\n",
    "losses_tr = []\n",
    "losses_te = []\n",
    "\n",
    "# cross validation\n",
    "for degree in degrees:\n",
    "    #for lambda_ in lambdas:\n",
    "    for gamma in gammas:\n",
    "        w = np.random.rand(tX_hot.shape[1])\n",
    "        for step in range(steps):\n",
    "            #loss, w = learning_by_newton_method(y_train, x_train, w, gamma)\n",
    "            loss, w = learning_by_gradient_descent(y_train, x_train, w, gamma)\n",
    "            losses_tr.append(loss)\n",
    "            losses_te.append(calculate_loss(y_test, x_test, w))\n",
    "\n",
    "        ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317570.8065528114\n",
      "319424.7696473628\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(14,14)) # change this if needed\n",
    "\n",
    "print(np.sqrt(2*compute_mse(y_train,x_train,w)))\n",
    "print(np.sqrt(2*compute_mse(y_test,x_test,w)))\n",
    "#degree 1\n",
    "ax1 = f.add_subplot(311)\n",
    "ax1.plot(range(steps), losses_tr, 'magenta', label=\"rmse_tr_deg1\")\n",
    "ax1.plot(range(steps), losses_te, 'brown', label=\"rmse_te_deg1\")\n",
    "\n",
    "plt.xlabel(\"gamma\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "print(losses_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,weights = ridge_regression(y, build_poly(tX_hot,2), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "67\n",
      "[-0.02881102  0.02839658 -0.09620074  0.01582389  0.02994869  0.01897818\n",
      "  0.01551189 -0.01681681  0.03502173 -0.00520658  0.02453951 -0.04923816\n",
      "  0.055845    0.0322811   0.06126586 -0.00014922 -0.00076271 -0.00134555\n",
      "  0.00035609  0.00021221 -0.00692964  0.00146511  0.01689059  0.00538524\n",
      "  0.02215877  0.00029284  0.00062506  0.00316131  0.00043526 -0.00082017\n",
      " -0.01437147  0.          0.          0.         -0.01216517  0.00737225\n",
      " -0.00540085  0.01863263  0.0177559   0.00118585  0.00065214 -0.05290048\n",
      " -0.00304813 -0.00399403  0.00065838  0.0008476   0.0077674  -0.0020547\n",
      " -0.04020606 -0.02433755  0.00155005 -0.05186904 -0.02601186  0.00214445\n",
      " -0.0228176  -0.01901872 -0.03734142 -0.00584636  0.02883954 -0.02068768\n",
      "  0.00119158  0.01426812 -0.00631553 -0.01437147  0.          0.\n",
      "  0.        ]\n",
      "0.34899460689176715\n"
     ]
    }
   ],
   "source": [
    "print(tX_hot.shape[1])\n",
    "print(len(weights))\n",
    "print(weights)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least_squares_DG\n",
    "\n",
    "## Least_squares_SDG\n",
    "Ne donne que des loss = nan, le calcul du gradient ne se passe pas bien, que a soit avec les rows contenant du -999 ou pas.\n",
    "\n",
    "## Least_square\n",
    "degree 1 -> -334.4 (mse loss)\n",
    "\n",
    "## Ridge_regression\n",
    "Seems the most suited since we have so many features. Maybe we could remove some features if they are highly correlated (might be visible on plots?\n",
    "degree = 6 -> singular matrix\n",
    "\n",
    "## Logistic_regression\n",
    "Since we have a binary output, it might be the best option...\n",
    "\n",
    "## Reg_logistic_regression\n",
    "\n",
    "# Remarks\n",
    "if degree = 3, 7, ...\n",
    "-> LinAlgError: Singular matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- plot all the variable with boxplot\n",
    "- compute ,np.cov between the variables\n",
    "- compute Pearson correlation np.cov(x,y)/(std(x) * std(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(w, x):\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '/home/toinou/course/ml/ml_proj1/data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/home/toinou/course/ml/ml_proj1/sub6.csv'\n",
    "tX_test.T[[22, -1]] = tX_test.T[[-1, 22]]\n",
    "tX_test_hot = one_hot_jet_num(clean(standardize(tX_test)))\n",
    "tX_test_hot = build_poly(tX_test_hot,2)\n",
    "y_pred = predict_labels(weights, tX_test_hot)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.53225415e-03,  2.53940620e-04, -9.08223071e-03, -2.12791035e-03,\n",
       "       -2.21631010e-03, -1.27817484e-03,  5.50923985e-04, -9.85283433e-03,\n",
       "        1.88673236e-02,  5.48501086e-05,  3.06728169e-03, -2.00786362e-02,\n",
       "        3.78874608e-02,  8.31814806e-03,  6.25395724e-03, -6.13018578e-04,\n",
       "       -1.08780006e-03,  2.37444582e-03, -9.39576560e-04,  1.05833265e-03,\n",
       "        4.94737608e-03,  7.16301000e-04, -7.86342456e-04, -5.56063433e-03,\n",
       "        1.63376716e-03, -8.09613631e-04, -6.88979545e-04,  9.23605741e-05,\n",
       "        1.87868848e-03,  4.48825670e-04,  2.79801857e-06, -5.35682988e-06,\n",
       "       -2.15119070e-03, -5.37850464e-03])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "33\n",
      "30\n",
      "33\n",
      "***\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "print(tX.shape[1])\n",
    "print(tX_hot.shape[1])\n",
    "print(tX_test.shape[1])\n",
    "print(tX_test_hot.shape[1])\n",
    "print(\"***\")\n",
    "print(len(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.98001e+05 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [9.98001e+05 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [9.98001e+05 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
      " ...\n",
      " [9.98001e+05 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [9.98001e+05 0.00000e+00 1.00000e+00 0.00000e+00 0.00000e+00]\n",
      " [9.98001e+05 1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00]]\n",
      "568238\n",
      "67\n",
      "250000\n",
      "[[6.125625e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00]\n",
      " [9.980010e+05 0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [9.980010e+05 0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00]\n",
      " ...\n",
      " [9.980010e+05 0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [9.980010e+05 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]\n",
      " [9.980010e+05 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(tX_test_hot[:,-5:])\n",
    "print(tX_test_hot.shape[0])\n",
    "print(tX_test_hot.shape[1])\n",
    "print(tX_hot.shape[0])\n",
    "print(build_poly(tX_hot,2)[:,-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submissions\n",
    "\n",
    "1. ridge, degree 2, lambdas = 0.75 -> 0.752\n",
    "2. ridge, degree 1, lambdas = 0.75 -> 0.732\n",
    "3. logreg, degree 1, lambda = 0.75 -> 0.662\n",
    "\n",
    "implementation of normalization and hot one\n",
    "\n",
    "4. ridge, degree 1, lambdas = 0.69 -> 0.752\n",
    "5. ridge, degree 2, lambdas = 0.69 -> 0.732\n",
    "6. logreg, degree 1, gamma ?       -> (random start, 3000 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pyt3.8",
   "language": "python",
   "name": "pyt3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
